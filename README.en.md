# üéôÔ∏è Podcast Reels Forge

## Automatically create Reels/Shorts from podcasts (local-first)

[![Python 3.10+](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)

**English** | [–†—É—Å—Å–∫–∏–π](README.md)

---

## üìå Table of Contents

- [What it does](#what-it-does)
- [Key Features](#key-features)
- [Quick Start](#quick-start)
- [Pipeline Overview](#pipeline-overview)
- [Output Layout](#output-layout)
- [Command Line Arguments](#command-line-arguments)
- [Configuration (config.yaml)](#configuration-configyaml)
- [Face-Aware Smart Crop](#face-aware-smart-crop-optional)
- [Rerendering Videos](#re-render-video-from-existing-momentsjson)
- [Performance and Stability](#performance-and-stability)
- [License](#license)

---

## What it does

**Podcast Reels Forge** is a powerful CLI tool designed to automatically extract viral short-form content (Reels, Shorts, TikTok) from long-form podcasts or interviews. It handles everything from speech recognition to final video editing.

Main workflow steps:

1. **Speech Recognition (Whisper)**: Converts audio/video into text with precise timestamps.
2. **Diarization (Optional)**: Identifies different speakers throughout the audio.
3. **AI Analysis (LLM)**: Discovers the most engaging and viral moments, generating catchy titles and hooks.
4. **Video Editing (FFmpeg)**: Cuts the video, applies vertical cropping (9:16), and centers onto faces.

Detailed user guide: [docs/USER_GUIDE.md](docs/USER_GUIDE.md)

---

## Key features

- **Batch Processing**: Drop multiple videos into `input/`, and the forge will process them all sequentially.
- **Multi-Model Support**: Integrated with **Ollama** for local models (qwen3, deepseek, gemma) and cloud providers (OpenAI, Anthropic, Gemini).
- **Smart Face Crop**: Automatically detects faces and centers the frame during vertical cropping.
- **Hardware Acceleration**: Uses **CUDA** for Whisper and **NVENC** for high-speed video rendering.
- **Ollama Watchdog**: Monitors model responsiveness and automatically retries/restarts stalled generations.
- **Flexible Clip Types**: Configure specific counts and durations for Stories, Reels, and Highlights separately.

---

## Quick start

### Requirements

- **Python 3.10+**
- **FFmpeg** (must be in PATH)
- **Ollama** (for local LLM support)
- **NVIDIA GPU** (highly recommended for performance)

### Installation

1. Clone the repository.
2. Create a virtual environment and install dependencies:

```bash
python3 -m venv whisper-env
source whisper-env/bin/activate
pip install -r requirements.txt
```

### Prepare Input

Place your video files (mp4, mkv, mov) in the `input/` directory.
*Tip: If you have a separate high-quality audio file with the same name, the script will prefer it for better transcription.*

### Run

```bash
python3 start_forge.py
```

---

## Pipeline overview

The orchestrator [start_forge.py](start_forge.py) runs [podcast_reels_forge/pipeline.py](podcast_reels_forge/pipeline.py), which executes the following stages for each file:

1. **Transcription**: Uses `faster-whisper`. Output: `output/<file_stem>/<audio_name>.json`.
2. **Diarization**: (If enabled) Creates `diarization.json`.
3. **Analyze (Parallel)**: Analyzes the transcript using all models specified in the config. Each model stores results in `output/<file_stem>/<model_name>/`.
4. **Video Processing**: Cuts clips based on the `moments.json` generated by each model.


---

## Output layout

Inside the `output/` directory:

```text
output/
  my_podcast/
    video.json            # Transcript
    diarization.json      # (Optional) Speaker info
    qwen3/                # Qwen model results
      moments.json        # List of viral moments
      reels.md            # Clip descriptions
      reels/              # Cut video clips .mp4
      reels_preview.mp4   # Concatenated preview of all clips
    deepseek/             # DeepSeek model results
      ...
```

---

## Command line arguments

Main flags for `start_forge.py`:

- `--config <path>`: Path to config (default: `config.yaml`).
- `--verbose`: Verbose output for all commands and logs.
- `--quiet`: Errors-only mode.
- `--no-skip-existing`: Rerun all stages even if files already exist (ignore cache).
- `--autotune`: Automatically tune parameters for current hardware (smaller chunks, longer timeouts).
- `--no-progress`: Disable progress bar (useful for CI/logging).

---

## Configuration (config.yaml)

### Key Sections

- **`transcription`**: Choose Whisper model (`medium`, `large-v3`), device (`cuda`/`cpu`), and language.
- **`ollama`**:
  - `models`: List of models for parallel analysis.
  - `watchdog`: Timeout settings for stall detection.
  - `model_overrides`: Model-specific tweaks for timeouts and chunking.
- **`processing`**: Set counts and durations for clip types (`stories`, `reels`, `highlights`).
- **`video`**:
  - `vertical_crop`: Enable/disable 9:16 aspect ratio.
  - `smart_crop_face`: Enable smart centering on faces.
  - `use_nvenc`: Use NVIDIA hardware acceleration.
- **`diarization`**: Enable and configure speaker detection (requires HuggingFace token).

---

## Face-aware smart crop (optional)

When `smart_crop_face: true` is enabled in config:

1. Several frames are sampled from each clip.
2. OpenCV Haar cascades are used to detect faces.
3. If faces are found, the 9:16 window is shifted to center the speaker.
4. If no faces are found, it falls back to a standard center crop.

---

## Re-render video from existing moments.json

If you want to change video parameters (bitrate, crop, padding) without re-running the long AI analysis, use [rerender_videos.py](rerender_videos.py):

```bash
# Re-render everything with smart crop enabled
python3 rerender_videos.py --smart-crop-face --replace
```

---

## Performance and stability

- **Whisper**: If you hit Out of Memory (OOM) errors, use `small` or `base` models.
- **Ollama**: If a model takes too long to respond, increase `first_token_timeout` in config.
- **FFmpeg**: If encoder errors occur, try disabling `use_nvenc` in config to use CPU encoding instead.

---

## License

MIT License ‚Äî see [LICENSE](LICENSE).
